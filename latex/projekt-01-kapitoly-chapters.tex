%=========================================================================
% (c) Michal Bidlo, Bohuslav Křena, 2008

\chapter{Úvod}
Na úvod mé práce uvedu několik důvodů, proč se zajímám o problematiku zpracování
rozsáhlých kolekcí textů z webu a také uvedu čtenáře do problematiky.
\section{Volba tématu}
Téma Rozšíření systému pro získávání, zpracování a analýzu rozsáhlých kolekcí textů z webu
jsem si zvolil hlavně kvůli své dvouleté spolupráci s výzkumnou skupinou KNOT \cite{KNOT}, svému zájmu a
svým zkušenostem v této problematice.
\section{Úvod do problematiky}
Cílem tohoto projektu není vytvořit nový proces zpracování webových korpusů, ale rozšířit stávající
systém. Aktuálně dokážeme jednorázově vytvořit korpus a sémanticky ho označit. Problém tedy není v tom,
že korpus nelze vytvořit, ale že to nelze provádět automaticky. To znamená, že do procesu musí vstoupit
člověk, který je obeznámen s nástroji i problematikou, spustit mnoho nástrojů ve správném pořadí a
sledovat, zda při zpracování nenastane chyba.

Dalšími problémemy jsou rozmanitost webových stránek, jak z hlediska vzhledu, jejich účelu, tak i
způsobu, jak je zdrojový kód stránky napsán. Dalším faktorem ovlivňující obtížnost je tolerance jazyka HTML k chýbám
Vícejazyčnost stránek nutí použití dalších nástrojů pro detekci jazyka, které zpomalují celkové zpracování
a tyto nástroje jsou často mateny ne tolik gramatickými chybami, jako spíše příliš krátkými textovými úseky,
jakým může být například komentář uživatele k jednomu ze článků na webové stránce.

Webových zdrojů je obrovské množství a je nemožné, nebo přinejmenším velice obtížné, napsat optimální program
pro všechny webové stránky. Má práce se zaměřuje zejména na zpracování článků v českých mediích. Velké množství těchto zdrojů
je logicky členěno do bloků, ale je zde i mnoho reklam. HTML kód je často bez chyb a lze ho poměrně jednoduše
zpracovat, ale jsou zde i skutečnosti, na které je potřeba si dát pozor.


\chapter{Kolekce odkazů a stahování webových stránek} % idealne 5 stran
Aby bylo možné zpracovat webové články, je nutné mít k dispozici alespoň minimum dat. Způsobů, pomocí kterých lze získávat pravidelně nové webové
články, je více. Oblíbeným je například klasický webcrawling, kde se ze stažené stránky extrahují odkazy a
získané adresy se použijí k dalšímu sběru dat. Má práce se zaměřuje hlavně na využití technologií RSS a ATOM.

RSS \textit{Rich Site Summary} je technologie sloužící k syndikaci obsahu. Využívá XML formátu a uživatelé
webových stránek si mohou díky této technologii nastavit odběr novinek z webu \cite{RSS}. Zejména díky tomu, že technologie RSS neměla ve
své době téměř žádnou konkurenci, rychle se rozšířila a ještě dnes je nejpoužívanějším nástrojem, přestože je vývoj od roku 2003
zastaven \cite{ATOM_VS_RSS}.

ATOM \textit{Atom Syndication Format} je jedním z webových standardů. Podobně, jako RSS, se využívá XML formátu a slouží k
informování uživatelů stránek o nových článcích \cite{ATOM} Na rozdíl od RSS se jedná o mladší formát a už od začátku se dbalo na
modularitu, znovupoužitelnost a přehlednost kódu. Zatímco RSS není IETF standardem, ATOM sem byl zařazen v prosinci roku 2005 \cite{ATOM_VS_RSS}.

\section{Zpracování RSS a ATOM souborů}
Jak RSS, tak i ATOM jsou psané pomocí jazyka XML. Proto jsem použil modul \textit{xml.etree.ElementTree} určený k práci nad jednotlivými
elementy. Ovšem ne všechny zpracovávané dokumenty jsou bez chyb a právě na tyto dokumenty je výše zmíněný modul krátký. U poškozených
dokumentů zpracování probíhá pomocí regulárních výrazů a díky tomu jsem schopen získat alespoň pár odkazů na nové články.

\section{Použité nástroje}
Kolekce odkazů a stahování stránek bylo naprogramováno v jazyce Python. Konkrétní verze jazyka je Python3.5, která je nainstalovaná na serverech,
na nichž budou nástroje spuštěny. Jazyk Python jsem zvolil nejen kvůli popularitě a velkému množství knihoven, ale i kvůli nenáročnosti
procesu kolekce a stahování stránek na procesor.

\label{urllib}
\subsection{Urllib}
Urllib je balíček několika modulů určených pro práci s url. Ve své práci jsem využil pouze modulu \textit{urllib.request},
který je určený ke stahování webových zdrojů pomocí url odkazu \cite{URLLIB}. Pomocí tohoto modulu se mi podařilo stáhnout většinu
odkazů, ale stále zde bylo několik stránek, které se podařilo stáhnout až na pozdější pokus a také zde byly stránky, které
se stáhnout nepodařilo, přestože přes prohlížeč na ně přistoupit bylo možné. Tento problém vyřešil nástroj \textit{GNU Wget}.

\label{wget}
\subsection{GNU Wget}
\textit{GNU Wget} je aplikace pro stahování souborů pomocí protokolů HTTP, HTTPS, FTP a FTPS. Je to neinteraktivní nástroj spouštěný uvnitř
příkazové řádky a je vhodný pro použití v automatických skriptech \cite{WGET}. Pro spuštění této aplikace uvnitř skriptu využívám knihovny \textit{subprocess}.
Bohužel úplné nahrazení \textit{urrlib} aplikací \textit{GNU Wget} poměrně výrazně zpomalilo proces kolekce odkazů. Bylo nutné každé stránce dát určitý časový limit
na stažení. Většina stránek je stažena do 1 sekundy, ale pokud chci stáhnout i zbylé stránky, které jsou umístěny na pomalejších, méně dostupných
nebo vzdálených serverech, je nezbytné časový limit zvýšit alespoň na 3 sekundy.

\section{Paralelní přístup}
Jak bylo zmíněno výše, aplikace \textit{GNU Wget} nemohla úplně nahradit \textit{urllib}, kvůli časové náročnosti. Proto byla zvolena zlatá střední cesta a v případě,
že \textit{urllib} selže, stránku se pokusila stáhnout aplikace \textit{GNU Wget}. Sice jsem dosáhl rychlejšího stahování, ale stále to nestačilo. Protože se ale většinu
času čeká na odpověď serveru a proces je uspaný, rozhodl jsem se zvolit paralelní přístup. Místo toho, aby na odpověď čekal jeden proces, čekalo na ni
hned několik procesů. K paralelizaci jsem využil modulu \textit{concurrent.futures} a dosáhl jsem několikanásobného zrychlení. Veškerá rizika,
která s sebou přináší paralelní zpracování, byla vyřešebńa přidělením maximální doby pro zpracování jednoho odkazu. Pokud rodič čekal příliš
dlouho na potomka, potomek byl zabit a rodič pokračoval v práci.


\section{Deduplikace}
Při zpracování rozsáhlých webových kolekcích je potřeba provádět deduplikaci dat. Každou staženou webovou stránku je potřeba zpracovat
dalšími nástroji a to znamená, že jim musíme věnovat určitý procesorový čas. Navíc je zbytečné archivovat jednu a tu samou stránku
vícekrát, proto je potřeba už při stahování článku dbát důraz na to, abychom opakovaně nestáhli tutéž stránku znovu.
Je třeba si uvědomit, že v této fázi celého procesu zpracování máme pouze odkazy a jediný možný způsob deduplikace dat je zpracovat každý
odkaz právě jednou. V novém systému se deduplikace řeší na úrovni kolekce odkazů. Kolektor odkazů si vede soubory, ve kterých jsou seznamy již
zpracovaných odkazů. Klade se přitom důraz na to, aby s tímto souborem manipuloval vždy právě jeden kolektor a sám kolektor si tyto soubory
u sebe držel minimální dobu a nedocházelo tak ke zbytečné ztrátě deduplikačních dat a poté i tvorbě následných duplicit.

\section{Ukládání stažených dat}
Stažené stránky se dlouhodobě ukládají a proto je potřeba zvolit vhodnou formu komprese dat. Dnes existuje spousta nástrojů umožňující
kompresi dat, k obecně nejznámějším patří asi WinRAR, 7-zip nebo gzip. Použití WinRAR a 7-zip je přinejmenším nevhodné v tomto projektu,
proto jsem se rozhodoval mezi výše zmíněným nástrojem gzip a ne tolik známým xz. Oba nástroje mají své výhody.
Mezi výhody formátu xz patří menší výsledná velikost souborů, ovšem za cenu rychlosti. Formát gzip má na druhou stanu
relativně rychlou kompresi a dekompresi dat \cite{GZIP_VS_XZ}. Protože, jak již bylo zmíněno,
se stránky budou ukládat dlouhodobě, zvolil jsem formát xz.

\section{Srovnání s původním systémem}
V této kapitole budete obeznámeni se základnímy prvky jak původního, tak nového systému a jejich zásadní rozdíly.

\subsection{Původní systém}
Původní systém využíval stahování stránek pouze pomocí \textit{urllib}. Kolekci odkazů z RSS a Atom zdrojů zastávaly 2
skripty -- \textit{collect.py} a \textit{download.py}. Skripty byly napsány v jazyce Python verze Python2.7 pro projekt
Newsbrief eu \cite{NEWSBRIEF}.

Kolekce odkazů byla implementovaná skriptem \textit{collect.py}. Samotná kolekce se spouštěla pomocí nástroje \textit{cron} každé
2 hodiny. Později byl skript přepsán do jazyka Python verze Python3.5 a optimalizován. To vystačilo až do doby, než se začaly hromadit
RSS zdroje a skript přestával stíhat sbírat odkazy z RSS a ATOM zdrojů. Stávalo se běžně, že v jednu chvíli běželo najednou několik kolekcí odkazů
a navzájem si přepisovaly deduplikační soubory a tak se jedna stránka mohla stáhnout vícekrát. Tehdy přestal být tento nástroj vhodný
pro svůj účel.

O stahování stránek se staral skript \textit{download.py}. Stejně jako \textit{collect.py}, i tento nástroj se spouštěl pravidelně
pomocí \textit{cronu}, akorát se namísto každé 2 hodiny spouštěl jednou za 24 hodin. Skript byl následně přepsán do jazyka Python3.6, kvůli chybě
ve vertikalizátoru \ref{vertikalizator}.

\subsection{Nový systém}
Momentálně se, na rozdíl od předchozích nástrojů, stahují stránky paralelně. O spouštění kolekce, stahování stránek, deduplikaci odkazů a vedení statistik se
stará skript \textit{big\_brother.py}, který pravidelně spouští 2 skripty. Prvním z nich je skript \textit{link\_collector.py}, využívající třídy \textit{Link\_collector}.
Tato třída je schopna vyhledat RSS a ATOM zdroje na stažených stránkách a extrahovat z RSS a ATOM souborů odkazy na nové články.
Navíc je tato třída přímým potomkem třídy \textit{Page\_downloader}, a díky dědičnosti je schopna stahovat stránky jak sekvenčně, tak paralelně.

Stahování je řešeno skriptem \textit{page\_downloader.py}, který využívá třídy \textit{Page\_generator}. Tato třída je schopna se dotazovat jednotlivých serverů
s časovým odstupem mezi dvěma dotazy na tentýž server a stahovat z nich stránky. Navíc je, stejně jako \textit{Link\_collector.py}, přímým potomkem
třídy \textit{Page\_downloader} a získává tak možnost paralelního i sekvenčního stahování souborů ze serverů.

\chapter{Získání čistého textu ze stažených stránek}
Ve chvíli, kdy jsou k dispozici data, lze započít další zpracování. Pro práci s html souborem existuje celá řada nástrojů
programovaných v mnoha jazycích, přesto ale je zde velké množství překážek.

\section{Problémy při zpracování webových stránek}
Jak již bylo dříve zmíněno, internet je velice rozmanitý a existuje nespočet způsobů, jak lze vytvořit webovou stránku.

\subsection{Kódování obsahu}
Kódováním obsahu se zde nemyslí šifrování, ale způsob, jakým jsou datově reprezentovány jednotlivé znaky. Podle dostupných
statistik se s drtivou převahou používá kódování UTF-8 -- až 91 \% stránek. Další 4 \% využívají ISO-8859-1 a 1 \%
vývojářů zvolilo kódování Windows-1251 \cite{W3TECHS}. Zbylá procenta obsahují ostatní kódování, kterých je nepřeberné množství.

Pří použití nástroje \textit{urllib} \ref{urllib} není problém zjistit, jaké kódování stránka používá. Protože je ale použit
i nástroj \textit{GNU WGET} \ref{wget}, je potřeba kódování vyčíst z hlavičky HTTP odpovědi serveru. Z vlastní zkušenosti však mohu říct, že
ne vždy je možné tímto způsobem kódování zjistit. Tento problém se vyskytl zatím pouze při zpracování starších archivů a to pouze ojediněle.

V případě, že se nepovede kódování obsahu zjistit, je využito modulu \textit{chardet}, který je schopen rozpoznat
hned několik kódování. V případě, že i tento nástroj selže, aplikace není schopna stránku zpracovat a stránka je zahozena.

\subsection{Reklamy}
Spousta stránek obsahuje reklamy. Reklama nemusí vůbec rušit strukturu obsahu, ale také může být vložena doprostřed
odstavce. Právě v této chvíli je schopná zmást jakýkoliv nástroj, který pracuje s obsahem stránky,
například proces analýzy textu nebo vyhledávač.

\subsubsection{Detekce reklam}
V dnešní době je spousta nástrojů, které jsou schopny detekovat reklamy na stránkách a odstranit je. Mezi nejznámější
patří například Adblock. Nejen Adblock, ale i jemu podobné nástroje pracují s množinou pravidel. Při zpracování HTML
elementu, zejména \textit{<img>} (obrázky) a \textit{<div>} (blokový element), zjistí identifikátor a třídu daného elementu a na
základě pravidel prvek zobrazí či nikoli. V případě externích odkazů (element \textit{<a>}) se zjišťuje adresa \cite{ADBLOCK}.

V této práci bylo hned několik pokusů o využití těchto nástrojů nebo jen použití samotných pravidel. Problémem byla náročnost procesu
na výpočetní výkon. Celý proces zpracování se zpomalil a výsledky nebyly tak dobré, jak se očekávalo. Z těchto důvodů tyto nástroje
nebyly vůbec použity.

\subsubsection{Rozdělení obsahu do logických celků}
Webové stránky, zejména zpravodajské deníky, mají text rozdělený do určitých bloků, za účelem
znovupoužitelnosti a přehlednosti kódu, co se uživatelů týče, tak i přívětivějšího vzhledu a
snadnějšího použití.

Nové nástroje jsou schopny takového členení využít a díky tomu označit pro nástroje,
které s textem budou nadále pracovat, kdy text pokračuje a kdy se text týká už něčeho jiného.
Toto řešení sice není tak dokonalé jako odstranění celé reklamy z textu, ale jeho náročnost na výpočetní
výkon je přijatelná a svůj účel to dokáže splnit. HTML elementy, pomocí
kterých se rozděluje text do logických bloků, jsou \textit{<p>} (odstavce),
\textit{<div>} (blokové elementy) \textit{<h1>, <h2>, <h3>, <h4> a <h5>} (nadpisy).

\subsection{Použité nástroje}
Pro extrakci textu z webových stránek bylo vyzkoušeno poměrně několik postupů a nástrojů. Co se týče použitých
programovacích jazyků, jendalo se o C a Python.

\subsubsection{Jednoduchý parser napsaný v jazyce C}
Vytvořit jednoduchý a rychlý program v jazyce C byla první myšlenkou a pokusem. Vytvořit
nástroj na extrakci textu z HTML kódu není zas tolik složité, dokud není potřeba detekovat reklamy nebo
členit text do logických bloků. Dalším problémem byly HTML entity, které je třeba nahradit za reálné znaky.

\subsubsection{Beatiful soup}
\textit{Beatiful soup} je knihovna pro jazyk Python určená k extrakci dat z HTML a XML dokumentů \cite{BEATIFULSOUP}.
Použit9 této knihovny je relativně jednoduché a dokumentace je výborná. Pokud by bylo v budoucnu potřeba použít pravidla pro
blokování reklam, je ideální volbou. Tato knihovna plnila svůj účel dlouhou dobu, přestože její slabina byla už od samého začátku
zřejmá -- rychlost. Jakmile se projekt rozšířil o další zpracování, byl modul, používající tuto knihovnu, brzdou v celém procesu
a bylo zřejmé, že je potřeba najít efektivnější řešení.

\subsubsection{Regulární výrazy}
Jednoduchou a relativně efektivní cestou se ukázalo použití regulárních výrazů. Sice se zkomplikovala detekci reklam na stránkách,
ale v době, kdy byly regulární výrazy zapojeny do procesu zpracování, bylo jasné, že se žádné rozpoznávání reklam
a jejich odstranění kvůli výkonosti provádět nebude.

\subsubsection{MyHTML}
Pro zajímavost je zde uveden ještě modul \textit{MyHTML} od Alexandera Borisova. Celý modul je psán v jazyce C,
konkrétně standard C99 \cite{MyHTML}. Tento modul do procesu nebyl nikdy zapojen, kvůli nedostatku času -- narazil jsem na něj
příliš pozdě. Navíc má velké nedostatky v dokumentaci a ke konfiguraci nástroje a samotného sestavení aplikace je
potřeba oprávnění administrátora. Program byl proto sestavin na osobním počítači, kde byl i vyzkoušen. Modul je poměrně rychlý
a je jednou možných z alternativ, které by mohli v budoucnu nahradit alespoň částečné použití regulárních výrazů.

\subsection{Původní systém}
V původním systému je k tomuto účelu použita aplikace zvaná vertikalizátor \ref{vertikalizator}, který využívá
mimo jiné nástroje \textit{jusText}.

\subsubsection{Nástroj jusText}
\textit{jusText} je nástroj určený k odstranění boilerplate obsahu, například navigační odkazy, hlavička HTML stránky.
Nástroj je vhodný pro použití ve zpracování HTML stránek \cite{JUSTEXT}. Potenciální nevýhodou tohoto nástroje je nutné mít tzv. \textit{stoplist}
na základě kterého se vyhodnocuje, zda je daný blok textu boilerplate -- text je nechtěný v dalším zpracování. \textit{Stoplist}
obsahuje slova (\textit{stop words}), která se snaží nástroj natít v textovém bloku. Na základě množství těchto slov v text,
je danému bloku přidělena třída, pomocí které lze určit, zda text je vhodné pro další zpracování text zachovat nebo je lepší
text odstranit \cite{JUSTEXT_ALG}.



\chapter{Převod textu do vertikálu} %idealne 10 stran
\subsection{Použité nástroje}
\subsection{Paralelní zpracování}
\label{vertikalizator}
\subsection{Srovnání se starým systémem}
\chapter{Tagging} % idealne 10 stran
\subsection{Použité nástroje}
\chapter{Závěr}
%=========================================================================
